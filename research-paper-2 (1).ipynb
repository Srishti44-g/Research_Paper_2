{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":6184937,"datasetId":3549529,"databundleVersionId":6264076},{"sourceType":"datasetVersion","sourceId":2010231,"datasetId":1202939,"databundleVersionId":2049747}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Objective : To Classify the video into normal , panic , violent , congestion and obstacle.","metadata":{}},{"cell_type":"markdown","source":"#### DATASETS\n1. UMN\n2. UCSD\n3. CUHK Avenue\n4. PETS2009\n5. ShanghaiTech\n6. UCF-crime\n7. PETS\n8. Violent Flow\n9. SHT Anamoly","metadata":{}},{"cell_type":"markdown","source":"## Literature Review Papers\n1. Recent Deep Learning in Crowd Behaviour Analysis : A Brief Review\nhttps://arxiv.org/pdf/2505.18401\n- Springer Nature [ January 2025 ]\n\n2. Design of an improved graph-based model for real time threat detection and dyanimic evacuation management using crowd behaviour analysis\nhttps://link.springer.com/article/10.1007/s41870-025-02489-x\n  - Springer Nature [March 2025]\n\n3. Human crowd behaviour analysis based on video segmentation and classification using expectationâ€“maximization with deep learning architectures\n - Springer Nature [March 2024]\n\n4. Convolutional Neural Networks for crowd behaviour aanlaysis : a survey\nhttps://link.springer.com/article/10.1007/s00371-018-1499-5 [March 2018]\n\n5. Crowd Emotion and Behavior Analysis Using LightWeight CNN Model\nhttps://www.internationaljournalssrg.org/IJEEE/paper-details?Id=863\n- Internationjournal [October 2024]\n","metadata":{}},{"cell_type":"markdown","source":"### Previous Methods \n1. Traditional Machine Learning Methods\n2. Deep Learning Methods\n  - 2.1 Recurrent Neural Network (RNN)\n  - 2.2 Convolutional Neural Network (CNN)\n  - 2.3 Graph Neural Network (GNN)\n  - 2.4 Generative Models\n  - 2.5 Transformers\n \n - Physics-inspired Deep Learning\n","metadata":{}},{"cell_type":"markdown","source":"# Research Papers\n1. Crowd Behaviour Representation : An attribute-based approach\nhttps://link.springer.com/article/10.1186/s40064-016-2786-0\n2. Crowd Behaviour Analysis : Survey\n3. Crowd Behaviour Monitoring and Analysis in Surveillance Applications : A Survey\n4. Self-supervised multi-view multi-lable learning with attention mechanisms\n5. Detecting violent and abnormal crowd actrivity using temporal analysis of grey level co-occurence matrix (GLCM) based texture measures\n6. Revisiting crowd behaviour analysis through deep learning : taxonomy , anomaly detection , crowd emotions , datasets , opportunities and prospects\n7. Recent trends in crowd analysis : A review\n8. Recent Deep Learning in Crowd Behaviour Analaysis : A Brief Review\n9. Multi-stage attention for efficient brain tumor classification wth SAMMed2D\n10. Self-supervised multi-view multi-lable learning with attention mechanism\n11. Crowd Behaviour Detection : leveraging video swin transformer for crowd size and violence analysis","metadata":{}},{"cell_type":"markdown","source":"## Research Papers (Technical Papers)\n1. Attention is all you need (2017)\nhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n2. Swin Transformer : Hierarchial Vision Transformer using Shifted Windows\n- https://ieeexplore.ieee.org/document/9710580\n- https://github.com/microsoft/Swin-Transformer\n- https://huggingface.co/docs/transformers/en/index\n- https://arxiv.org/html/2408.13609v1\n- https://research.google/blog/graph-neural-networks-in-tensorflow/\n- https://github.com/thunlp/GNNPapers\n- https://arxiv.org/abs/2412.08016\n- https://arxiv.org/abs/2202.02093 [https://ieeexplore.ieee.org/document/10191427 ]\n- https://github.com/PangzeCheung/OmniTransfer","metadata":{}},{"cell_type":"markdown","source":"#### METHODS OR FRAMEWORKS\n1. CNN Convolutional Neural Network\n2. Swin transformer\n3. Spatio Temporal\n4. Graph Construction Layer\n5. Graph Neural Network\n6. Temporal Attention Model\n7. Multimodal Fusion\n","metadata":{}},{"cell_type":"markdown","source":"### Proposed Methodology\n1. Overview\n2. Data Processing and Input Representation\n    - 2.1 Video Processing\n   -  2.2 Multimodal Inputs\n3 Spatial Feature Extraction [CNN+ Swin Transformer]\n    - 3.1 CNN-Based Local Feature Extraction\n    - 3.2 Global Context Modeling using Swin Transformer\n4. Spatio-Temporal Feature Modeling\n5. Graph Construction Layer\n    - 5.1 Graph Representation\n    - 5.2 Node Feature Assignment\n6. Graph Neural Network [GNN]\n7. Temporal Attention Mechanism\n8. Multimodal Feature Fusion\n9. Classification and Output Layer","metadata":{}},{"cell_type":"markdown","source":"### Parameters and Hyperparameters (optional)\n\n#### Hyperparameters\n1. Input Size\n2. Initial learning rate\n3. Learning Rate Update Frequency\n4. Momentum\n5. Batch Size\n6. Weight Decay\n7. No. of Frames in a sample\n\n\n#### Losses\n1. Cross-entropy\n2. Contrastive loss\n3. Temporal Smoothness loss\n\n#### Optimization\n1. AdamW\n2. Warmup\n3. Cosine LR","metadata":{}},{"cell_type":"code","source":"#!pip install timm einops torch-geometric --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:16:54.157338Z","iopub.execute_input":"2026-01-30T12:16:54.157541Z","iopub.status.idle":"2026-01-30T12:16:54.161105Z","shell.execute_reply.started":"2026-01-30T12:16:54.157521Z","shell.execute_reply":"2026-01-30T12:16:54.160593Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# ENVIORNMENTAL SETUP","metadata":{}},{"cell_type":"markdown","source":"- https://pypi.org/project/opencv-python/\n- GitHub Link (Open CV) : https://github.com/opencv/opencv\n-  OpenCV : https://opencv.org/\n-  PyTorch : https://pytorch.org/\n-  https://pypi.org/project/torch/\n-  PyTorch Image Models : https://github.com/huggingface/pytorch-image-models","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport timm\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom einops import rearrange\nfrom torchvision import transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T11:34:47.472424Z","iopub.execute_input":"2026-02-09T11:34:47.472685Z","iopub.status.idle":"2026-02-09T11:35:04.170472Z","shell.execute_reply.started":"2026-02-09T11:34:47.472662Z","shell.execute_reply":"2026-02-09T11:35:04.169847Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONFIGURATION","metadata":{}},{"cell_type":"code","source":"class CFG:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    img_size = 224\n    frames = 16\n    batch_size = 4   # GPU-safe\n    epochs = 30\n    lr = 1e-4\n    num_classes = 5  # Normal / Abnormal\n    weight_decay = 0.01\n    base_path = \"kaggle/input/avenue-dataset/avenue/avenue\"\n\ncfg = CFG()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATASET","metadata":{}},{"cell_type":"code","source":"class VideoFrameDataset(Dataset):\n    def __init__(self, root, transform=None):\n        self.samples = []\n        self.transform = transform\n\n        for label, cls in enumerate([\"normal\", \"abnormal\"]):\n            cls_path = os.path.join(root, cls)\n            if not os.path.exists(cls_path):\n                continue\n            for vid in os.listdir(cls_path):\n                frames = sorted(os.listdir(os.path.join(cls_path, vid)))\n                self.samples.append((os.path.join(cls_path, vid), frames, label))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, frames, label = self.samples[idx]\n        selected = frames[:cfg.frames]\n\n        clip = []\n        for f in selected:\n            img = cv2.imread(os.path.join(path, f))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (cfg.img_size, cfg.img_size))\n            if self.transform:\n                img = self.transform(img)\n            clip.append(img)\n\n        clip = torch.stack(clip)\n        return clip, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.358764Z","iopub.execute_input":"2026-01-30T12:17:06.359081Z","iopub.status.idle":"2026-01-30T12:17:06.369841Z","shell.execute_reply.started":"2026-01-30T12:17:06.359047Z","shell.execute_reply":"2026-01-30T12:17:06.369255Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406],\n                         std=[0.229,0.224,0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.370836Z","iopub.execute_input":"2026-01-30T12:17:06.371074Z","iopub.status.idle":"2026-01-30T12:17:06.380846Z","shell.execute_reply.started":"2026-01-30T12:17:06.371046Z","shell.execute_reply":"2026-01-30T12:17:06.380172Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# VIDEO PROCESSING\n1. Video Acquistion / Input\n2. Frame Extraction\n3. Frame Sampling / Temporal Sampling\n4. Frame Resizing\n5. Frame Normalization\n6. Data Augmentation\n7. Frame Stacking / Tensor Formation\n8. Temporal Alignment\n9. Feature Extraction\n10. Label Encoding and Dataset Formatting","metadata":{}},{"cell_type":"code","source":"#importing necessary libraries for video processing\nimport os\nimport cv2\nimport numpy as np\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.382868Z","iopub.execute_input":"2026-01-30T12:17:06.383097Z","iopub.status.idle":"2026-01-30T12:17:06.392848Z","shell.execute_reply.started":"2026-01-30T12:17:06.383077Z","shell.execute_reply":"2026-01-30T12:17:06.392241Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Configuration\nFRAME_SIZE = (224, 224)\nCLIP_LENGTH = 16\nCLIP_STRIDE = 1\n\nMEAN = np.array([0.485, 0.456, 0.406])\nSTD  = np.array([0.229, 0.224, 0.225])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.393638Z","iopub.execute_input":"2026-01-30T12:17:06.393940Z","iopub.status.idle":"2026-01-30T12:17:06.406270Z","shell.execute_reply.started":"2026-01-30T12:17:06.393919Z","shell.execute_reply":"2026-01-30T12:17:06.405640Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## LOAD VIDEO","metadata":{}},{"cell_type":"code","source":"def load_video(video_path):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    if not cap.isOpened():\n        raise IOError(f\"Cannot open video {video_path}\")\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frames.append(frame)\n\n    cap.release()\n    return np.array(frames)  # (T, H, W, C)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.407269Z","iopub.execute_input":"2026-01-30T12:17:06.407513Z","iopub.status.idle":"2026-01-30T12:17:06.418185Z","shell.execute_reply.started":"2026-01-30T12:17:06.407493Z","shell.execute_reply":"2026-01-30T12:17:06.417724Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## TEMPORAL SAMPLING","metadata":{}},{"cell_type":"code","source":"def temporal_sampling(frames, num_frames):\n    total = len(frames)\n\n    if total <= num_frames:\n        return frames\n\n    indices = np.linspace(0, total - 1, num_frames).astype(int)\n    return frames[indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.419222Z","iopub.execute_input":"2026-01-30T12:17:06.419540Z","iopub.status.idle":"2026-01-30T12:17:06.428692Z","shell.execute_reply.started":"2026-01-30T12:17:06.419510Z","shell.execute_reply":"2026-01-30T12:17:06.428100Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## RESIZE FRAMES","metadata":{}},{"cell_type":"code","source":"def resize_frames(frames, size):\n    resized = []\n    for frame in frames:\n        resized.append(\n            cv2.resize(frame, size, interpolation=cv2.INTER_LINEAR)      )\n    return np.array(resized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.429505Z","iopub.execute_input":"2026-01-30T12:17:06.429828Z","iopub.status.idle":"2026-01-30T12:17:06.439462Z","shell.execute_reply.started":"2026-01-30T12:17:06.429808Z","shell.execute_reply":"2026-01-30T12:17:06.438870Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## NORMALIZE PIXEL VALUE","metadata":{}},{"cell_type":"code","source":"def normalize_frames(frames, mean, std):\n    frames = frames.astype(np.float32) / 255.0\n    return (frames - mean) / std","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.440306Z","iopub.execute_input":"2026-01-30T12:17:06.440802Z","iopub.status.idle":"2026-01-30T12:17:06.449786Z","shell.execute_reply.started":"2026-01-30T12:17:06.440780Z","shell.execute_reply":"2026-01-30T12:17:06.449265Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## SLIDING WINDOW CLIP GENERATION","metadata":{}},{"cell_type":"code","source":"def generate_clips(frames, clip_length, stride):\n    clips = []\n\n    for start in range(0, len(frames) - clip_length + 1, stride):\n        clip = frames[start:start + clip_length]\n        clips.append(clip)\n\n    return np.array(clips)  # (N, T, H, W, C)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.450521Z","iopub.execute_input":"2026-01-30T12:17:06.450750Z","iopub.status.idle":"2026-01-30T12:17:06.461030Z","shell.execute_reply.started":"2026-01-30T12:17:06.450731Z","shell.execute_reply":"2026-01-30T12:17:06.460482Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## FRAME TO TENSOR CONVERSION","metadata":{}},{"cell_type":"code","source":"def to_tensor(clip):\n    clip = torch.tensor(clip, dtype=torch.float32)\n    return clip.permute(3, 0, 1, 2)  # (C, T, H, W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.461941Z","iopub.execute_input":"2026-01-30T12:17:06.462310Z","iopub.status.idle":"2026-01-30T12:17:06.471986Z","shell.execute_reply.started":"2026-01-30T12:17:06.462281Z","shell.execute_reply":"2026-01-30T12:17:06.471400Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## LOAD FRAME LEVEL GROUND TRUTH ","metadata":{}},{"cell_type":"code","source":"def load_ground_truth(gt_file):\n    return np.loadtxt(gt_file, dtype=int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.472855Z","iopub.execute_input":"2026-01-30T12:17:06.473309Z","iopub.status.idle":"2026-01-30T12:17:06.482677Z","shell.execute_reply.started":"2026-01-30T12:17:06.473281Z","shell.execute_reply":"2026-01-30T12:17:06.482079Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## FRAME --> CLIP LABEL CONVERSION","metadata":{}},{"cell_type":"code","source":"def frame_to_clip_labels(frame_labels, clip_length, stride):\n    clip_labels = []\n\n    for start in range(0, len(frame_labels) - clip_length + 1, stride):\n        clip_labels.append(\n            int(frame_labels[start:start + clip_length].max())\n        )\n\n    return np.array(clip_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:17:06.483575Z","iopub.execute_input":"2026-01-30T12:17:06.483853Z","iopub.status.idle":"2026-01-30T12:17:06.493188Z","shell.execute_reply.started":"2026-01-30T12:17:06.483821Z","shell.execute_reply":"2026-01-30T12:17:06.492563Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def preprocess_video(video_path, gt_path=None, is_train=True):\n    frames = load_video(video_path)\n    frames = resize_frames(frames, FRAME_SIZE)\n    frames = normalize_frames(frames, MEAN, STD)\n\n    clips = generate_clips(frames, CLIP_LENGTH, CLIP_STRIDE)\n\n    if is_train:\n        labels = np.zeros(len(clips), dtype=int)\n    else:\n        frame_labels = load_ground_truth(gt_path)\n        labels = frame_to_clip_labels(\n            frame_labels, CLIP_LENGTH, CLIP_STRIDE\n        )\n\n    clips = [to_tensor(clip) for clip in clips]\n    return clips, labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_path = \"Avenue/testing/videos/01.avi\"\ngt_path    = \"Avenue/testing/ground_truth/01_gt.txt\"\n\nclips, labels = preprocess_video(\n    video_path,\n    gt_path=gt_path,\n    is_train=False\n)\n\nprint(\"Total clips:\", len(clips))\nprint(\"Clip shape:\", clips[0].shape)\nprint(\"First 10 labels:\", labels[:10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN BACKBONE","metadata":{}},{"cell_type":"markdown","source":"- CNN Wikipedia : https://en.wikipedia.org/wiki/Convolutional_neural_network\n- Stanford : https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks/#inception-network\n- Resnet : https://huggingface.co/microsoft/resnet-50\n- Keras : https://keras.io/api/applications/resnet/\n- Deep Residual Learning for Image Recognition : https://arxiv.org/abs/1512.03385\n- https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html","metadata":{}},{"cell_type":"code","source":"class CNNBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"resnet50\",\n            pretrained=True,\n            features_only=True\n        )\n\n    def forward(self, x):\n        return self.backbone(x)[-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:45:47.190520Z","iopub.execute_input":"2026-01-28T09:45:47.190785Z","iopub.status.idle":"2026-01-28T09:45:47.201816Z","shell.execute_reply.started":"2026-01-28T09:45:47.190766Z","shell.execute_reply":"2026-01-28T09:45:47.201247Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# SWIN TRANSFORMER","metadata":{}},{"cell_type":"markdown","source":"- Model : (Hugging face model link ) : https://huggingface.co/microsoft/swin-tiny-patch4-window7-224\n- Swin Transformer: Hierarchical Vision Transformer using Shifted Windows : https://arxiv.org/abs/2103.14030\n- Swin Transformer (Microsoft) : https://github.com/microsoft/Swin-Transformer\n- Swin Transformer (Hugging Face) : https://huggingface.co/docs/transformers/v4.39.1/model_doc/swin\n- Image Classification with Swin Transformer : https://keras.io/examples/vision/swin_transformers/\n- Swin Transformer V2: Scaling Up Capacity and Resolution : https://ieeexplore.ieee.org/document/9879380\n- https://www.microsoft.com/en-us/research/blog/swin-transformer-supports-3-billion-parameter-vision-models-that-can-train-with-higher-resolution-images-for-greater-task-applicability/","metadata":{}},{"cell_type":"code","source":"class SwinBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.swin = timm.create_model(\n            \"swin_tiny_patch4_window7_224\",\n            pretrained=True,\n            num_classes=0\n        )\n\n    def forward(self, x):\n        return self.swin(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:45:47.203260Z","iopub.execute_input":"2026-01-28T09:45:47.203785Z","iopub.status.idle":"2026-01-28T09:45:47.213365Z","shell.execute_reply.started":"2026-01-28T09:45:47.203763Z","shell.execute_reply":"2026-01-28T09:45:47.212783Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# SPATIO TEMPORAL ","metadata":{}},{"cell_type":"markdown","source":"- Link :  https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html\n- Keras : https://keras.io/api/layers/convolution_layers/convolution3d/","metadata":{}},{"cell_type":"code","source":"class SpatioTemporal(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.conv3d = nn.Conv3d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = rearrange(x, \"b t c h w -> b c t h w\")\n        x = self.conv3d(x)\n        return rearrange(x, \"b c t h w -> b t c h w\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:45:47.214150Z","iopub.execute_input":"2026-01-28T09:45:47.214402Z","iopub.status.idle":"2026-01-28T09:45:47.225417Z","shell.execute_reply.started":"2026-01-28T09:45:47.214382Z","shell.execute_reply":"2026-01-28T09:45:47.224809Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"#  GRAPH CONSTRUCTION LAYER","metadata":{}},{"cell_type":"markdown","source":"### Research Paper\n1. Design of an improved graph-based model for real-time threat detection and dyanmic evacuation management using crowd behaviour analysis. Link : https://link.springer.com/article/10.1007/s41870-025-02489-x\n2. Human crowd behaviour analysis based on video segmentation and classification using expectation - maximation with deep learning architectures Link : https://link.springer.com/article/10.1007/s11042-024-18630-0\n3. Recent Deep Learning in Crowd Behaviour Analysis : A Brief Review Link : https://arxiv.org/pdf/2505.18401\n4. Crowd Abnormal Behaviour Detection and Comparative Analysis using YOLO Network Link : http://ieeexplore.ieee.org/document/10543372\n5. The Battle of Westminster : Developing the social identity model of crowd behaviour in order to explain the initation and development of collective conflict. Link : https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-0992(199601)26:1%3C115::AID-EJSP740%3E3.0.CO;2-Z\n6. Convolutional neural networks for crowd behaviour analysis : a survey Link : https://link.springer.com/article/10.1007/s00371-018-1499-\n7. A novel framework and concept -based semantic search interface for abnormal crpwd behaviour analysis in surveillance videos Link : https://link.springer.com/article/10.1007/s11042-020-08659-2\n8. Crowd 11 : A Dataset for Fine Grained Crowd Behvaiour Analysis Link : https://openaccess.thecvf.com/content_cvpr_2017_workshops/w37/papers/Dupont_Crowd-11_A_Dataset_CVPR_2017_paper.pdf\n9. Crowd Behavioural Analysis at a Mass Gathering Event Link :\n10. High-Level Feature Extraction for Crowd Behaviour Analysis : A Computer Vision Approach Link : https://link.springer.com/chapter/10.1007/978-3-031-13324-4_6\n11. Crowd Emotion and Behavior Analysis Using Lightweight CNN model Link : https://www.internationaljournalssrg.org/IJEEE/paper-details?Id=863\n12. Agile-LSTM : Acclimatizing Convolution Neural Network for Crowd Behaviour Analysis Link : https://link.springer.com/chapter/10.1007/978-981-16-1249-7_31\n13. Violent Behaviour Analysis in Crowd  Link : https://ieeexplore.ieee.org/document/10493819\n\n## Recent Papers\n1. Exploring the role of layer variations in ANN Crowd Behaviour and Prediction Accuracy Link : https://www.cambridge.org/core/journals/proceedings-of-the-design-society/article/exploring-the-role-of-layer-variations-in-ann-crowd-behaviour-and-prediction-accuracy/402123BAB20E6C7A1D02CF763EF6B222\n2. Automated Crowd Abnormality Detection and Segmentation Using Machine Learning Techniques Link : https://ieeexplore.ieee.org/document/10915397\n3. Identification odf crowd behaviour patterns using stability analysis","metadata":{}},{"cell_type":"markdown","source":"- torch.cdist : https://docs.pytorch.org/docs/stable/generated/torch.cdist.html\n- torch.topk : https://docs.pytorch.org/docs/main/generated/torch.topk.html","metadata":{}},{"cell_type":"code","source":"def build_knn_graph(x, k=5):\n    dist = torch.cdist(x, x)\n    return dist.topk(k, largest=False).indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:45:47.272162Z","iopub.execute_input":"2026-01-28T09:45:47.272416Z","iopub.status.idle":"2026-01-28T09:45:47.276423Z","shell.execute_reply.started":"2026-01-28T09:45:47.272393Z","shell.execute_reply":"2026-01-28T09:45:47.275730Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# GRAPH NEURAL NETWORK","metadata":{}},{"cell_type":"code","source":"class SimpleGNN(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim)\n\n    def forward(self, x):\n        return F.relu(self.fc(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:45:47.277852Z","iopub.execute_input":"2026-01-28T09:45:47.278110Z","iopub.status.idle":"2026-01-28T09:45:47.288713Z","shell.execute_reply.started":"2026-01-28T09:45:47.278090Z","shell.execute_reply":"2026-01-28T09:45:47.288160Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# TEMPORAL ATTENTION","metadata":{}},{"cell_type":"code","source":"class TemporalAttention(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(dim, 4, batch_first=True)\n\n    def forward(self, x):\n        out, _ = self.attn(x, x, x)\n        return out.mean(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:45:47.289548Z","iopub.execute_input":"2026-01-28T09:45:47.289735Z","iopub.status.idle":"2026-01-28T09:45:47.300804Z","shell.execute_reply.started":"2026-01-28T09:45:47.289717Z","shell.execute_reply":"2026-01-28T09:45:47.300251Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"class CrowdBehaviorModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cnn = CNNBackbone()\n        self.swin = SwinBackbone()\n        self.st = SpatioTemporal(1024)\n        self.gnn = SimpleGNN(1024)\n        self.temporal = TemporalAttention(1024)\n        self.fc = nn.Linear(1024, cfg.num_classes)\n\n    def forward(self, x):\n        B, T, C, H, W = x.shape\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n\n        cnn_feat = self.cnn(x)\n        swin_feat = self.swin(x).unsqueeze(-1).unsqueeze(-1)\n\n        feat = cnn_feat + swin_feat\n        feat = rearrange(feat, \"(b t) c h w -> b t c h w\", b=B)\n\n        feat = self.st(feat)\n        feat = feat.mean([-1, -2])\n\n        feat = self.gnn(feat)\n        feat = self.temporal(feat)\n\n        return self.fc(feat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:45:47.301603Z","iopub.execute_input":"2026-01-28T09:45:47.301860Z","iopub.status.idle":"2026-01-28T09:45:47.314456Z","shell.execute_reply.started":"2026-01-28T09:45:47.301839Z","shell.execute_reply":"2026-01-28T09:45:47.313812Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"# TRAINING LOOP","metadata":{}},{"cell_type":"markdown","source":"- PyTorch : https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n- Keras : https://keras.io/api/optimizers/adamw/\n- Cross Entropy Loss\n- PyTorch : https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n- Keras (Probabilistic losses ) : https://keras.io/api/losses/probabilistic_losses/","metadata":{}},{"cell_type":"code","source":"model = CrowdBehaviorModel().to(cfg.device)\noptimizer = torch.optim.AdamW(\n    model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:45:47.315903Z","iopub.execute_input":"2026-01-28T09:45:47.316126Z","iopub.status.idle":"2026-01-28T09:45:48.665934Z","shell.execute_reply.started":"2026-01-28T09:45:47.316106Z","shell.execute_reply":"2026-01-28T09:45:48.665287Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.optim.lr_scheduler import CosineAnnealingLR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:02:57.167909Z","iopub.execute_input":"2026-01-28T10:02:57.168607Z","iopub.status.idle":"2026-01-28T10:02:58.583124Z","shell.execute_reply.started":"2026-01-28T10:02:57.168576Z","shell.execute_reply":"2026-01-28T10:02:58.582535Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# --- 1. DATA PREPARATION ---\ntrain_dataset = AvenueDataset(cfg.base_path, split=\"training\", transform=transform)\ntest_dataset = AvenueDataset(cfg.base_path, split=\"testing\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n\n# --- 2. MODEL, LOSS, & OPTIMIZER ---\nmodel = CrowdBehaviorModel().to(cfg.device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1) # Improved for generalization\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\nscheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n\n# --- 3. TRAINING & VALIDATION LOOP ---\ndef train_one_epoch(model, loader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    for x, y in loader:\n        x, y = x.to(cfg.device), y.to(cfg.device)\n        optimizer.zero_grad()\n        outputs = model(x)\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate(model, loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(cfg.device), y.to(cfg.device)\n            outputs = model(x)\n            probs = F.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(y.cpu().numpy())\n            all_probs.extend(probs[:, 1].cpu().numpy()) # Probability for 'Abnormal' class\n            \n    return all_labels, all_preds, all_probs\n\n# --- 4. EXECUTION ---\nprint(\"Starting Training...\")\nfor epoch in range(cfg.epochs):\n    avg_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n    scheduler.step()\n    print(f\"Epoch {epoch+1}/{cfg.epochs} | Loss: {avg_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n\n# Final Evaluation\nlabels, preds, probs = evaluate(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:03:01.521452Z","iopub.execute_input":"2026-01-28T10:03:01.522267Z","iopub.status.idle":"2026-01-28T10:03:01.532743Z","shell.execute_reply.started":"2026-01-28T10:03:01.522237Z","shell.execute_reply":"2026-01-28T10:03:01.531863Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2465056964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- 1. DATA PREPARATION ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAvenueDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAvenueDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AvenueDataset' is not defined"],"ename":"NameError","evalue":"name 'AvenueDataset' is not defined","output_type":"error"}],"execution_count":49},{"cell_type":"code","source":"def plot_results(labels, preds, probs):\n    # 1. Confusion Matrix\n    cm = confusion_matrix(labels, preds)\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Abnormal'], yticklabels=['Normal', 'Abnormal'])\n    plt.title('Confusion Matrix')\n    plt.show()\n\n    # 2. AUC-ROC Curve\n    \n    auc = roc_auc_score(labels, probs)\n    print(f\"\\nArea Under Curve (AUC): {auc:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(labels, preds, target_names=['Normal', 'Abnormal']))\n\nplot_results(labels, preds, probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:04:41.362145Z","iopub.execute_input":"2026-01-28T10:04:41.362467Z","iopub.status.idle":"2026-01-28T10:04:41.370313Z","shell.execute_reply.started":"2026-01-28T10:04:41.362441Z","shell.execute_reply":"2026-01-28T10:04:41.369441Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1581491692.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"],"ename":"NameError","evalue":"name 'labels' is not defined","output_type":"error"}],"execution_count":50},{"cell_type":"markdown","source":"### Swin Transformer (Shifted Window Transformer)\n\nOverview of Swin Transformer\n\nArchitecture of Swin Transformer\n1. Patch Splitting\n2. Window-Based Self-Attention\n3. Shifted Windows for Cross-Region Interaction","metadata":{}},{"cell_type":"markdown","source":"### Implementation of Swin Tranformer","metadata":{}},{"cell_type":"markdown","source":"#### Step 1. Setup Environment","metadata":{}},{"cell_type":"code","source":"#!pip install transformers datasets torch torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:41:57.257315Z","iopub.execute_input":"2026-01-29T05:41:57.257667Z","iopub.status.idle":"2026-01-29T05:41:57.261345Z","shell.execute_reply.started":"2026-01-29T05:41:57.257623Z","shell.execute_reply":"2026-01-29T05:41:57.260693Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#### Step 2. Import Libraries","metadata":{}},{"cell_type":"code","source":"from transformers import AutoImageProcessor, SwinForImageClassification\nfrom datasets import load_dataset\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:44:32.548438Z","iopub.execute_input":"2026-01-29T05:44:32.548782Z","iopub.status.idle":"2026-01-29T05:44:32.552847Z","shell.execute_reply.started":"2026-01-29T05:44:32.548757Z","shell.execute_reply":"2026-01-29T05:44:32.552015Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:44:30.289776Z","iopub.execute_input":"2026-01-29T05:44:30.290450Z","iopub.status.idle":"2026-01-29T05:44:30.294153Z","shell.execute_reply.started":"2026-01-29T05:44:30.290419Z","shell.execute_reply":"2026-01-29T05:44:30.293596Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"####  Load Pre-Trained Model","metadata":{}},{"cell_type":"code","source":"model_name = \"microsoft/swin-tiny-patch4-window7-224\"\nimage_processor = AutoImageProcessor.from_pretrained(model_name)\nmodel = SwinForImageClassification.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:43:32.526050Z","iopub.execute_input":"2026-01-29T05:43:32.527030Z","iopub.status.idle":"2026-01-29T05:43:34.785125Z","shell.execute_reply.started":"2026-01-29T05:43:32.526996Z","shell.execute_reply":"2026-01-29T05:43:34.784567Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8962aceb579e4b519b3f5c4cf03d0b98"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d40e32cda54893a76a0a0de2fe8d35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/113M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c688cdad33aa4d9bb1b8049730f9167f"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Load Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"cifar10\", split=\"test[:8]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:45:36.215308Z","iopub.execute_input":"2026-01-29T05:45:36.216081Z","iopub.status.idle":"2026-01-29T05:45:39.829724Z","shell.execute_reply.started":"2026-01-29T05:45:36.216050Z","shell.execute_reply":"2026-01-29T05:45:39.829033Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"374b21bf0e5e4c35b4bc6dcc8b85084a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/120M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42bb7e3ce42447e9b549d248b34d76e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/23.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"361dc32aa41f439892b9b5f592ee46e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"417cd62a790a4acfa793cc9231b0173a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a87bbb204a744ef89ae96acbf267e6c8"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### Extract Images and Labels","metadata":{}},{"cell_type":"code","source":"images = [item[\"img\"] for item in dataset]\nlabels = [item[\"label\"] for item in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:52:39.017158Z","iopub.execute_input":"2026-01-29T05:52:39.017783Z","iopub.status.idle":"2026-01-29T05:52:39.027039Z","shell.execute_reply.started":"2026-01-29T05:52:39.017753Z","shell.execute_reply":"2026-01-29T05:52:39.026434Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Preprocess Images","metadata":{}},{"cell_type":"code","source":"inputs = image_processor(images, return_tensors=\"pt\").to(model.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:52:42.890929Z","iopub.execute_input":"2026-01-29T05:52:42.891672Z","iopub.status.idle":"2026-01-29T05:52:42.910236Z","shell.execute_reply.started":"2026-01-29T05:52:42.891618Z","shell.execute_reply":"2026-01-29T05:52:42.909671Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Classify Images","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:53:51.960692Z","iopub.execute_input":"2026-01-29T05:53:51.961498Z","iopub.status.idle":"2026-01-29T05:53:52.827435Z","shell.execute_reply.started":"2026-01-29T05:53:51.961459Z","shell.execute_reply":"2026-01-29T05:53:52.826746Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":" ### Process Predictions","metadata":{}},{"cell_type":"code","source":"predicted_labels = logits.argmax(dim=-1).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:54:35.626083Z","iopub.execute_input":"2026-01-29T05:54:35.626801Z","iopub.status.idle":"2026-01-29T05:54:35.632308Z","shell.execute_reply.started":"2026-01-29T05:54:35.626772Z","shell.execute_reply":"2026-01-29T05:54:35.631770Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Handle Label Mismatches","metadata":{}},{"cell_type":"code","source":"num_classes = len(model.config.id2label)\nif num_classes != len(set(labels)):\n    print(\"Warning: Model label space does not match CIFAR-10 labels. Mapping may be required.\")\n    class_mapping = {i: i % 10 for i in range(num_classes)}\n    predicted_labels = [class_mapping[label] for label in predicted_labels]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:55:01.177266Z","iopub.execute_input":"2026-01-29T05:55:01.178055Z","iopub.status.idle":"2026-01-29T05:55:01.182553Z","shell.execute_reply.started":"2026-01-29T05:55:01.178023Z","shell.execute_reply":"2026-01-29T05:55:01.181990Z"}},"outputs":[{"name":"stdout","text":"Warning: Model label space does not match CIFAR-10 labels. Mapping may be required.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Map Predictions to Class Names","metadata":{}},{"cell_type":"code","source":"class_names = [\n    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n]\npredicted_class_names = [class_names[label] for label in predicted_labels]\ntrue_class_names = [class_names[label] for label in labels]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:55:40.008245Z","iopub.execute_input":"2026-01-29T05:55:40.008571Z","iopub.status.idle":"2026-01-29T05:55:40.013810Z","shell.execute_reply.started":"2026-01-29T05:55:40.008540Z","shell.execute_reply":"2026-01-29T05:55:40.013113Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Print Results","metadata":{}},{"cell_type":"code","source":"for i, (true_label, predicted_label) in enumerate(zip(true_class_names, predicted_class_names)):\n    print(\n        f\"Image {i + 1}: True Label = {true_label}, Predicted Label = {predicted_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T05:56:19.239830Z","iopub.execute_input":"2026-01-29T05:56:19.240409Z","iopub.status.idle":"2026-01-29T05:56:19.244589Z","shell.execute_reply.started":"2026-01-29T05:56:19.240379Z","shell.execute_reply":"2026-01-29T05:56:19.243888Z"}},"outputs":[{"name":"stdout","text":"Image 1: True Label = cat, Predicted Label = dog\nImage 2: True Label = ship, Predicted Label = automobile\nImage 3: True Label = ship, Predicted Label = ship\nImage 4: True Label = airplane, Predicted Label = bird\nImage 5: True Label = frog, Predicted Label = bird\nImage 6: True Label = frog, Predicted Label = ship\nImage 7: True Label = automobile, Predicted Label = dog\nImage 8: True Label = frog, Predicted Label = automobile\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Advantages\n1. Efficient on High Resolution Images\n2. Versatile\n3. Reduced Computational Complexity\n4. Strong Real-World Performance","metadata":{}},{"cell_type":"markdown","source":"### Limitations\n1. Limited Global Context\n2. Increased Complexity\n3. Resource Demands for Large Models\n4. Weaker Local Inductive Bias","metadata":{}},{"cell_type":"code","source":"# Cell 4: Dataset Implementation\nclass AvenueDataset(Dataset):\n    def __init__(self, root, split=\"training\", transform=None):\n        self.samples = []\n        self.transform = transform\n        self.split = split\n        \n        split_path = os.path.join(root, split, \"frames\")\n        if not os.path.exists(split_path):\n            print(f\"Warning: Path {split_path} not found.\")\n            return\n\n        for vid_folder in sorted(os.listdir(split_path)):\n            vid_path = os.path.join(split_path, vid_folder)\n            frames = sorted([f for f in os.listdir(vid_path) if f.endswith(('.jpg', '.png'))])\n            \n            # Avenue Logic: Training is usually all Normal (0). \n            # Abnormalities (1) appear in Testing.\n            label = 0 if split == \"training\" else 1 \n            \n            if len(frames) >= cfg.frames:\n                self.samples.append((vid_path, frames, label))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, frames, label = self.samples[idx]\n        # Temporal Sampling: Select frames evenly across the clip\n        indices = np.linspace(0, len(frames) - 1, cfg.frames).astype(int)\n        \n        clip = []\n        for i in indices:\n            img = cv2.imread(os.path.join(path, frames[i]))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (cfg.img_size, cfg.img_size))\n            if self.transform:\n                img = self.transform(img)\n            clip.append(img)\n\n        return torch.stack(clip), label\n\n# Cell 5: DataLoaders\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_ds = AvenueDataset(cfg.base_path, split=\"training\", transform=transform)\ntest_ds = AvenueDataset(cfg.base_path, split=\"testing\", transform=transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\ntest_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}